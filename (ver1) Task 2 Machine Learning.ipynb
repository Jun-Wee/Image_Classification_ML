{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15d454d7",
   "metadata": {},
   "source": [
    "#### Image Classification \n",
    "##### Task2 : Machine Learning\n",
    "##### Student: Jun Wee Tan 101231636"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fccc7fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "#import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "637a621a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to load and serialize the CIFAR-10 dataset files\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eccdc1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Batch 1 length:  4\n",
      "Data Batch 1 keys:  dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "Data Batch 1 data file type: uint8\n",
      "Data Batch 1 data file shape: (10000, 3072)\n",
      "\n",
      "Data Batch 2 length:  4\n",
      "Data Batch 2 keys:  dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "Data Batch 2 data file type: uint8\n",
      "Data Batch 2 data file shape: (10000, 3072)\n",
      "\n",
      "Data Batch 3 length:  4\n",
      "Data Batch 3 keys:  dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "Data Batch 3 data file type: uint8\n",
      "Data Batch 3 data file shape: (10000, 3072)\n",
      "\n",
      "Data Batch 4 length:  4\n",
      "Data Batch 4 keys:  dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "Data Batch 4 data file type: uint8\n",
      "Data Batch 4 data file shape: (10000, 3072)\n",
      "\n",
      "Data Batch 5 length:  4\n",
      "Data Batch 5 keys:  dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "Data Batch 5 data file type: uint8\n",
      "Data Batch 5 data file shape: (10000, 3072)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 59,  43,  50, ..., 140,  84,  72],\n",
       "       [154, 126, 105, ..., 139, 142, 144],\n",
       "       [255, 253, 253, ...,  83,  83,  84],\n",
       "       ...,\n",
       "       [ 71,  60,  74, ...,  68,  69,  68],\n",
       "       [250, 254, 211, ..., 215, 255, 254],\n",
       "       [ 62,  61,  60, ..., 130, 130, 131]], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset checking and preparation\n",
    "batch_list = []\n",
    "\n",
    "#import all 5 data batches of image\n",
    "for i in range (1,6):  \n",
    "    db = unpickle(f'data_batch_{i}')\n",
    "    batch_list.append(db)\n",
    "\n",
    "#checking\n",
    "for index, batch  in enumerate(batch_list):\n",
    "    print(f\"Data Batch {index + 1} length: \", len(batch))\n",
    "    print(f\"Data Batch {index + 1} keys: \", batch.keys())\n",
    "    print(f\"Data Batch {index + 1} data file type:\", batch[b'data'].dtype)\n",
    "    print(f\"Data Batch {index + 1} data file shape:\", batch[b'data'].shape)\n",
    "    print()\n",
    "    \n",
    "batch_list[0][b'data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ab3f58c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#display image \n",
    "def plot_images(images, labels, num_images):\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(10, 2)) #create subplot with 1 row and 5 column\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(images[i])\n",
    "        ax.set_title(f\"Label/Catg: {labels[i]}\")\n",
    "        ax.axis('off') \n",
    "    plt.show()\n",
    "\n",
    "for batch in batch_list:\n",
    "    #reshape data from flatten array 10000x3072 to 10000x32x32x3 RGB images\n",
    "    images_reshaped = batch[b'data'].reshape(10000, 3, 32, 32).transpose(0, 2, 3, 1) #transpose into (10000x32x32x3) position\n",
    "    labels = batch[b'labels']\n",
    "    #plot_images(images_reshaped, labels, num_images=5) #show 5 image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8552f5d",
   "metadata": {},
   "source": [
    "#### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee8794da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min for batch1: 0.0\n",
      "Max for batch1: 1.5378700499807765e-05\n",
      "\n",
      "Min for batch2: 0.0\n",
      "Max for batch2: 1.5378700499807765e-05\n",
      "\n",
      "Min for batch3: 0.0\n",
      "Max for batch3: 1.5378700499807765e-05\n",
      "\n",
      "Min for batch4: 0.0\n",
      "Max for batch4: 1.5378700499807765e-05\n",
      "\n",
      "Min for batch5: 0.0\n",
      "Max for batch5: 1.5378700499807765e-05\n",
      "\n",
      "(10000, 3072) (10000,)\n",
      "Mean of scaled training data (should be close to 0): -4.2142770754575546e-16\n",
      "Standard deviation of scaled training data (should be close to 1): 0.9999999999999998\n"
     ]
    }
   ],
   "source": [
    "#min-max normalisation \n",
    "#normalise pixel values from [0,255] to [0, 1] in each data batch\n",
    "for index, batch in enumerate(batch_list):\n",
    "    #changed float to float32 to alleviate memory issues : Michael\n",
    "    batch[b'data'] = batch[b'data'].astype(float) / 255\n",
    "    print(f\"Min for batch{index+1}: {np.min(batch[b'data'])}\")\n",
    "    print(f\"Max for batch{index+1}: {np.max(batch[b'data'])}\")\n",
    "    print()\n",
    "\n",
    "#flatten\n",
    "#reshape each data batch\n",
    "for batch in batch_list:\n",
    "    batch[b'data'] = batch[b'data'].reshape(10000, 3072)  # Flatten the images back to 2D array\n",
    "\n",
    "#standardization \n",
    "#re-scales the data_batch to have the mean 0 and std 1\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#combine all 5 training data_batch into single array \n",
    "arrayX = [] \n",
    "arrayY = []\n",
    "for batch in batch_list:\n",
    "    arrayX.append(batch[b'data'])\n",
    "    arrayY.append(batch[b'labels'])\n",
    "\n",
    "#merge 5 array into single array\n",
    "#Commented out to stop memory issues\n",
    "#X_train = np.concatenate(arrayX, axis=0)\n",
    "#y_train = np.concatenate(arrayY, axis=0)\n",
    "#print(X_train.shape, y_train.shape)\n",
    "\n",
    "#Temporary Training Batch - Michael\n",
    "X_train = batch_list[0][b'data']\n",
    "y_train = batch_list[0][b'labels']\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "#initialize\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#train\n",
    "scaler.fit(X_train)\n",
    "\n",
    "#transform\n",
    "std_X_train = scaler.transform(X_train)\n",
    "print(\"Mean of scaled training data (should be close to 0):\", np.mean(std_X_train))\n",
    "print(\"Standard deviation of scaled training data (should be close to 1):\", np.std(std_X_train))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de4423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Transform and fit the rest of the batches\n",
    "#Other batches - Michael\n",
    "X_train_b1 = batch_list[1][b'data']\n",
    "y_train_b1 = batch_list[1][b'labels']\n",
    "X_train_b1 = np.array(X_train_b1)\n",
    "y_train_b1 = np.array(y_train_b1)\n",
    "print(X_train_b1.shape, y_train_b1.shape)\n",
    "#initialize\n",
    "\n",
    "#train\n",
    "scaler.fit(X_train_b1)\n",
    "\n",
    "#transform\n",
    "std_X_train_b1 = scaler.transform(X_train_b1)\n",
    "print(\"Mean of scaled training data (should be close to 0):\", np.mean(std_X_train_b1))\n",
    "print(\"Standard deviation of scaled training data (should be close to 1):\", np.std(std_X_train_b1))\n",
    "\n",
    "X_train_b2 = batch_list[2][b'data']\n",
    "y_train_b2 = batch_list[2][b'labels']\n",
    "X_train_b2 = np.array(X_train_b2)\n",
    "y_train_b2 = np.array(y_train_b2)\n",
    "print(X_train_b2.shape, y_train_b2.shape)\n",
    "#initialize\n",
    "\n",
    "#train\n",
    "scaler.fit(X_train_b2)\n",
    "\n",
    "#transform\n",
    "std_X_train_b2 = scaler.transform(X_train_b2)\n",
    "print(\"Mean of scaled training data (should be close to 0):\", np.mean(std_X_train_b2))\n",
    "print(\"Standard deviation of scaled training data (should be close to 1):\", np.std(std_X_train_b2))\n",
    "\n",
    "X_train_b3 = batch_list[3][b'data']\n",
    "y_train_b3 = batch_list[3][b'labels']\n",
    "X_train_b3 = np.array(X_train_b3)\n",
    "y_train_b3 = np.array(y_train_b3)\n",
    "print(X_train_b3.shape, y_train_b3.shape)\n",
    "\n",
    "#train\n",
    "scaler.fit(X_train_b3)\n",
    "\n",
    "#transform\n",
    "std_X_train_b3 = scaler.transform(X_train_b3)\n",
    "print(\"Mean of scaled training data (should be close to 0):\", np.mean(std_X_train_b3))\n",
    "print(\"Standard deviation of scaled training data (should be close to 1):\", np.std(std_X_train_b3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e596b6",
   "metadata": {},
   "source": [
    "#### Learning type of problem: \n",
    "* Image Classification based on image pixel feature with Machine Learning\n",
    "#### Machine Learning algorithm: Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbb425ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DUMP MODEL - Michael M\n",
    "###Gonna dump the model so you guys don't have to retrain it and destroy your laptops CPUs.\n",
    "load = True\n",
    "\n",
    "\n",
    "#https://stackoverflow.com/questions/56107259/how-to-save-a-trained-model-by-scikit-learn\n",
    "if load:\n",
    "    with open('svmModel.pkl', 'rb') as file:\n",
    "        svm_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edccd79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape: (10000, 3072)\n",
      "Training labels shape: (10000,)\n",
      "Initializing the SVM...\n",
      "Training the model...\n",
      "Making predictions...\n",
      "Validation Accuracy: 0.3055\n",
      "Confusion Matrix:\n",
      " [[ 80  10  27   6   7   7   6   8  43  13]\n",
      " [ 22  79  20  11   8   8   7  16  15  27]\n",
      " [ 15  17  50  23  32  12  14  15  15   4]\n",
      " [ 22  12  23  55  15  31  17  13   8   3]\n",
      " [ 14   7  44  21  49  27  23  13   5   2]\n",
      " [ 12  17  21  38  25  32  10   8   8   6]\n",
      " [  6   4  34  35  30  21  40   8   7   2]\n",
      " [ 18  13  31  17  18   9   9  71   7   7]\n",
      " [ 41  20  12   9  10   8   3   6 104  11]\n",
      " [ 25  41  11  11   9  10   7  10  16  51]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "print(\"Training features shape:\", std_X_train.shape)\n",
    "print(\"Training labels shape:\", y_train.shape)\n",
    "\n",
    "#split the data into 80% train and 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(std_X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "#initial\n",
    "print(\"Initializing the SVM...\")\n",
    "\n",
    "#If model hasn't been loaded, create new model\n",
    "if load == False:\n",
    "    #n_jobs = -1 means use all availible cores. You may want to change this ahaha.\n",
    "    svm_model = svm.SVC(kernel='linear', n_jobs=-1)\n",
    "\n",
    "#train\n",
    "print(\"Training the model...\")\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "print(\"Making predictions...\")\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Validation Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69ec33e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DUMP MODEL - Michael M\n",
    "###Gonna dump the model so you guys don't have to retrain it and destroy your laptops CPUs.\n",
    "dump = False\n",
    "###Change dump to True if you want to save the model since you've done extra training\n",
    "\n",
    "#https://stackoverflow.com/questions/56107259/how-to-save-a-trained-model-by-scikit-learn\n",
    "if dump:\n",
    "    with open('svmModel.pkl', 'wb') as file:\n",
    "        pickle.dump(svm_model, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "528d5760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 108 candidates, totalling 1080 fits\n"
     ]
    }
   ],
   "source": [
    "#### Image Classification - Model Trraining\n",
    "##### Task2 : Machine Learning\n",
    "##### Student: Michael Mooney 104266194\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "testSVM = True\n",
    "#Maybe make a pipelinbe here just for my sanity\n",
    "pipelineSVM = Pipeline(\n",
    "    [   \n",
    "        ('clf', svm_model)\n",
    "    ]\n",
    ")\n",
    "skfSVM = StratifiedKFold(n_splits=10)\n",
    "\n",
    "if testSVM:\n",
    "    params = {\n",
    "    'clf__kernel':['linear', 'rbf', 'poly'],\n",
    "    'clf__gamma':[0.01, 0.10, 0.5, 1,2,3],\n",
    "    'clf__C':[0.01, 0.1, 0.5, 1,2,3]\n",
    "    }\n",
    "    \n",
    "    grid = GridSearchCV(\n",
    "        pipelineSVM, \n",
    "        params, \n",
    "        cv = skfSVM, \n",
    "        refit = True, \n",
    "        verbose = 3, \n",
    "        n_jobs=-1, \n",
    "        error_score='raise'\n",
    "    )\n",
    "    grid.fit(\n",
    "        X_train, \n",
    "        y_train\n",
    "            )\n",
    "     \n",
    "    gridPred = grid.predict(X_test) \n",
    "\n",
    "    print(pipelineSVM.get_params().keys())\n",
    "    print(grid.best_params_)\n",
    "    print(grid.best_estimator_)\n",
    "    print(classification_report(y_test, gridPred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f7b660",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PIPELINE - RFC_CLF\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_RFC = RandomForestClassifier(random_state=37)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
